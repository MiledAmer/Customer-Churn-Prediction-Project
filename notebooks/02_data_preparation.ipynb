{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699951ba",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This section performs the entire data preparation pipeline:\n",
    "1. **Load, Clean, and Select Features**: Imports the dataset, cleans it, and selects relevant features using the `utils` package.\n",
    "2. **Stratified Split**: Splits the processed data into training and test sets while preserving the class distribution of the target variable (`Churn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdc8011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found: 0\n",
      "Missing values in TotalCharges: 11\n",
      "customerID dropped.\n",
      "Feature Selection: Dropped 'TotalCharges' (Multicollinearity).\n",
      "Current Shape: (7043, 19)\n",
      "Data Split Successfully (Test Size: 0.2)\n",
      " - Train Shape: (5634, 18)\n",
      " - Test Shape:  (1409, 18)\n",
      "\n",
      "Verifying Churn Rate Preservation:\n",
      "Train Churn Rate: 26.54%\n",
      "Test Churn Rate:  26.54%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "# Add utils to path\n",
    "sys.path.append('..')\n",
    "from utils import preprocessing as prep\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load, Clean, and Select Features\n",
    "# ---------------------------------------------------------\n",
    "df = prep.load_data('../data/dataset.csv')\n",
    "df_clean = prep.clean_data(df)\n",
    "\n",
    "# Use our new function to remove TotalCharges\n",
    "df_ready = prep.select_features(df_clean)\n",
    "\n",
    "print(f\"Current Shape: {df_ready.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Stratified Split\n",
    "# ---------------------------------------------------------\n",
    "# Use our new function to split the data\n",
    "X_train, X_test, y_train, y_test = prep.split_stratified_data(df_ready, target='Churn')\n",
    "\n",
    "# Verify the Churn Rate (Just to be sure)\n",
    "print(\"\\nVerifying Churn Rate Preservation:\")\n",
    "print(f\"Train Churn Rate: {y_train.value_counts(normalize=True)['Yes']:.2%}\")\n",
    "print(f\"Test Churn Rate:  {y_test.value_counts(normalize=True)['Yes']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac6b003",
   "metadata": {},
   "source": [
    "## 1. Target Encoding\n",
    "\n",
    "We must convert the target variable `Churn` from text into numbers so the model can calculate the error.\n",
    "* **Function:** `encode_binary_target`\n",
    "* **Logic:** Maps the positive class ('Yes') to `1` and the negative class ('No') to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff71ffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding: Mapping 'Yes' -> 1 and 'No' -> 0\n",
      "Target Encoding: Mapping 'Yes' -> 1 and 'No' -> 0\n",
      "\n",
      "Target Encoded Successfully.\n",
      "Train Class Distribution:\n",
      "Churn\n",
      "0    4139\n",
      "1    1495\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We use our generic function to map Yes->1 and No->0\n",
    "y_train_enc = prep.encode_binary_target(y_train, pos_label='Yes', neg_label='No')\n",
    "y_test_enc = prep.encode_binary_target(y_test, pos_label='Yes', neg_label='No')\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nTarget Encoded Successfully.\")\n",
    "print(f\"Train Class Distribution:\\n{y_train_enc.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f06cd2c",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering (The Pipeline)\n",
    "\n",
    "We build a **Feature Transformer** to process the input variables (`X`). This ensures the model receives mathematically consistent data.\n",
    "\n",
    "**The Strategy:**\n",
    "1.  **Numerical Columns (e.g., `tenure`, `MonthlyCharges`):**\n",
    "    * **Action:** Apply `StandardScaler`.\n",
    "    * **Reason:** Centers the data around 0 with a standard deviation of 1. This prevents large values (like TotalCharges) from dominating the model weights.\n",
    "\n",
    "2.  **Categorical Columns (e.g., `InternetService`):**\n",
    "    * **Action:** Apply `OneHotEncoder`.\n",
    "    * **Reason:** Converts text categories into binary columns (0s and 1s). We use `drop='first'` to avoid multicollinearity (the \"dummy variable trap\").\n",
    "\n",
    "**Important:** We fit the transformer *only* on the Training data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415b0fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Configuration:\n",
      " - Scaling 3 numerical cols: ['SeniorCitizen', 'tenure', 'MonthlyCharges']\n",
      " - Encoding 15 categorical cols: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "\n",
      "Feature Engineering Complete.\n",
      "Original Shape: (5634, 18)\n",
      "Processed Shape: (5634, 29)\n"
     ]
    }
   ],
   "source": [
    "# 1. Build the Transformer (Using our new function)\n",
    "# This identifies which columns are numbers vs. text automatically\n",
    "feature_transformer = prep.create_feature_transformer(X_train)\n",
    "\n",
    "# 2. Fit and Transform Train\n",
    "# The transformer learns the means (for scaling) and categories (for encoding) from Train\n",
    "X_train_enc = feature_transformer.fit_transform(X_train)\n",
    "\n",
    "# 3. Transform Test\n",
    "# We apply the EXACT same rules to Test (we do not re-fit!)\n",
    "X_test_enc = feature_transformer.transform(X_test)\n",
    "\n",
    "# 4. Convert to DataFrames for inspection\n",
    "# We recover the new column names generated by the OneHotEncoder\n",
    "feature_names = feature_transformer.get_feature_names_out()\n",
    "\n",
    "X_train_processed = pd.DataFrame(X_train_enc, columns=feature_names, index=X_train.index)\n",
    "X_test_processed = pd.DataFrame(X_test_enc, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(f\"\\nFeature Engineering Complete.\")\n",
    "print(f\"Original Shape: {X_train.shape}\")\n",
    "print(f\"Processed Shape: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eaab19",
   "metadata": {},
   "source": [
    "## 3. Saving Processed Data\n",
    "\n",
    "We save the processed datasets and the `feature_transformer` object.\n",
    "* **Datasets:** Saved as CSVs for the Modeling Notebook.\n",
    "* **Transformer:** Saved as a `.pkl` file. This is critical for the final application, allowing us to process new raw customer data exactly the same way as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a455dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../data/processed/\n",
      "Transformer saved to ../data/processed/feature_transformer.pkl\n"
     ]
    }
   ],
   "source": [
    "prep.save_processed_data(\n",
    "    X_train_processed, \n",
    "    X_test_processed, \n",
    "    y_train_enc, \n",
    "    y_test_enc, \n",
    "    feature_transformer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
